Technical Audit of the VerdeSat MVP Repository

1. Missing Features and Functionality

The current MVP implementation is focused on core data workflows but lacks several essential components planned for later phases. Notably, caching layers, cloud integration, and interactive tools are not yet implemented. The project README’s Phase 2 roadmap explicitly lists features like a data caching layer, interactive dashboards, advanced modeling (LSTM/Prophet), and cloud deployment (Helm/Kubernetes, Airflow workflows, cloud storage) which are absent in the current codebase . For example, all data outputs are written to local files, with no hooks for cloud storage or database persistence. There is no evidence of Earth Engine authentication management in the code – the user likely needs to manually authenticate via the GEE API before running, which would need to be addressed for cloud automation. Also, while the repo structure includes many domain-specific subpackages (biodiversity/, carbon_flux/, etc. ), these appear to be stubs with little to no implemented functionality (the Phase 1 deliverable was a “monorepo scaffold + stub modules” ). In summary, the MVP is missing:
    • Data caching and reuse: Each run likely re-fetches data from Google Earth Engine (GEE) without caching results. A caching layer (e.g. saving intermediate composites or time series) is noted as a future addition .
    • Cloud-ready I/O: There’s no abstraction for writing to cloud storage or databases – outputs go to local CSVs, images, and an HTML report. Features like using S3/GCS for input/output (planned in roadmap ) are not implemented.
    • User interface and API: The MVP provides a CLI, but no REST API or web interface. Interactive dashboard deliverables (Streamlit/Dash apps) are listed for later phases . As of now, non-technical users can’t easily interact with the system (no GUI).
    • Advanced analytics and modeling: Features like hyperparameter tuning, LSTM forecasting, and additional indices or datasets are only outlined conceptually. The codebase contains placeholders (empty modules for agri_health, biodiversity, etc.), indicating these capabilities are not implemented yet.
    • Robust error handling & validation: The MVP does not appear to include comprehensive input validation or error-handling for common issues (e.g. invalid GeoJSON inputs, network failures, missing data). These are essential for a production system but not fully present in the current code.

Overall, the MVP meets the minimal requirements (download NDVI time series, basic analysis, simple forecast) but significant functionality is deferred to future phases, including cloud deployment, interactive visualization, and more sophisticated analysis.

2. Code Quality: Best Practices and Gaps

The repository shows evidence of good intentions (the README even lists best practices like type hints, testing, and linting ), but the actual code adheres to these standards inconsistently. Below is an evaluation of various code-quality aspects and whether the project meets typical Python and software engineering best practices:
    • Project Structure & Packaging: The code is organized as a Python package (verdesat/ with submodules) and includes both a pyproject.toml and a setup.py for installation . Having both can be redundant – it’s usually better to rely on one standardized build system (e.g. Poetry with pyproject.toml or setuptools with setup.py). Care should be taken that the metadata (package name, entry points, dependencies) is consistent between them. The repository structure is logical and monorepo-style, separating concerns by domain (ingestion, analytics, modeling, etc.) . This modular layout is good, but many modules are nearly empty, which adds complexity without function. Until those modules have substantial code, it might simplify development to condense or remove unused scaffolding (to avoid confusion and dead code).
    • Naming Conventions: In general, naming is descriptive (e.g. export_composites_to_png in ingestion/chips.py clearly describes its purpose). CLI command functions follow verbs like download, analyze, forecast, which is intuitive. One inconsistency is some command functions are suffixed with *_cmd in code (e.g. fill_gaps_cmd) whereas others are not, which could be confusing. It’s better to use consistent naming (perhaps no suffix for functions that are invoked via CLI). Also, certain parameter names could be clearer – for instance, --agg for aggregation frequency is short; using a full name like --agg-frequency (with a short alias) might be more explicit. Overall, naming is not a major issue, but there is room to standardize and clarify names for commands and parameters.
    • Documentation & Docstrings: The code includes some docstrings and comments, but not universally. We see docstrings in core functions like the image export function and the pipeline command , describing their purpose and parameters. However, many CLI command functions likely lack detailed docstrings (relying on click’s help text for documentation). Important modules (e.g. any complex analysis logic) should start with module-level docstrings explaining their role. Likewise, public functions and classes should have docstrings explaining parameters and return values. The README provides a good high-level overview and quickstart, but in-code documentation could be expanded. Ensuring every function has at least a brief docstring (as recommended by the project’s own best-practice list ) will improve maintainability.
    • Type Hinting: The project has begun using type hints in function signatures, which is a great practice for clarity and potential static checking. For example, the export_composites_to_png function explicitly annotates parameters and return type . Commits like “chore: MyPy linter - fix types” suggest that type-checking was run and adjustments were made . That said, type hints should be consistent and comprehensive. It appears some parts of the codebase may still lack hints or use Any by default. It’s recommended to enforce hints on all functions and use mypy in CI to catch issues. Also consider using typing.Optional and other advanced types as needed (the code already uses Optional[float] in places ). One specific gap: the code likely interacts with external libraries (earthengine API, pandas DataFrames, etc.); adding hints for those (e.g. using pandas.DataFrame as a type) would further clarify interfaces. In summary, type hinting is partially implemented – continuing to add hints across the codebase and fixing any mypy warnings will align with best practices.
    • Exception Handling: The current code appears to use basic exception handling and in some cases simply lets errors propagate. There are checks that raise ValueError for invalid states (e.g. no features in a GeoJSON, or empty image collection) , which is good for catching problems early. However, catching and handling exceptions in user-facing commands is limited. For instance, if a network request to GEE fails or times out, or if an input file path is wrong, does the CLI catch this and print a user-friendly message? Presently, the CLI functions likely rely on Click to handle usage errors, but not runtime errors. The pipeline code shows multiple steps without a global try/catch; if one step fails, it might stop the entire pipeline without graceful cleanup or error messaging. It would improve robustness to wrap major CLI operations in try/except blocks and log errors clearly. Additionally, no custom exception classes are defined – introducing some (e.g. DataDownloadError, AnalysisError) could help differentiate and handle different failure modes. In short, the MVP should improve exception handling by catching foreseeable errors (file I/O issues, API errors, etc.), providing informative error messages, and ensuring partial results are handled or cleaned up if a later stage fails.
    • Logging and Debugging: The repository mentions logging in the core (and we see a commented-out logger.info call in the code ), but it’s unclear if a structured logging setup is in place. It would be prudent to configure Python’s logging module (or an external logger) to report progress and issues. For example, logging when data download starts/ends for each region, when analysis steps begin, etc., at INFO level, and detailed debug information (like the URL fetched from Earth Engine) at DEBUG level. The presence of commented debug statements suggests logging was considered but not fully implemented. Setting up a central logger in core/logging.py or similar, and using it throughout instead of print statements or silences, would follow best practices. This also ties into cloud readiness – in a cloud environment, console logs are crucial for monitoring, so robust logging will be needed.
    • Style and Linting: The commit history includes multiple formatting-related commits (e.g. “chore: black linter” on several dates ). This indicates the team is using Black for auto-formatting and likely flake8 or similar for linting (as stated in README best practices ). The presence of these commits is positive; it shows effort to conform to PEP8 and maintain a consistent style. Any remaining style issues are likely minor. One thing to ensure is that linting is run regularly (perhaps via a pre-commit hook as planned) so that dead code (like large commented blocks) or unused imports are caught and removed. For example, there are large sections of commented-out code in the pipeline command ; a linter could flag these if configured to discourage left-over code. Overall, aside from a few minor issues (commented code, possibly some long functions that could be refactored), code style is being handled appropriately with automated tools.
    • Modularity and Separation of Concerns: The code structure suggests an intent for modular design: ingestion for data fetching, analytics for calculations, visualization for output generation, etc. In practice, however, there is some blurring of concerns in this MVP. For instance, the CLI pipeline_report function orchestrates many steps (invoking download, aggregation, fill, decompose, plot, report) all in one place . This pipeline logic mixes concerns of data fetching, processing, and output in a single function. While it’s convenient for an MVP, in a cleaner design these could be separated: e.g. a higher-level controller or workflow class could coordinate calls to ingestion and analytics modules, rather than the CLI function directly handling it. Another example: the ingestion.chips module currently not only fetches images from GEE but also writes them to disk (using requests to download thumbnails ). This couples remote data access with local file I/O. A better separation would be to have ingestion return in-memory data or at least provide an abstraction for storage (so it could be a local file or cloud object storage). As the project grows, ensuring each module has a single responsibility will be key. Currently the code is small enough that the coupling is manageable, but the foundation should be improved to avoid tangling logic across modules.

In summary, the code quality is decent for an MVP, but there are clear areas to improve: enforce the best practices already identified (type hints, docs, testing, style) more uniformly, remove or resolve any half-implemented code sections, handle errors more gracefully, and continue refactoring to keep functions and modules focused and readable. Adopting these improvements will make the code more maintainable as the project scales.

3. System Architecture and Design Improvements

The overall architecture of VerdeSat’s MVP is a single Python application that orchestrates several tasks (data ingestion from Earth Engine, time series analysis, image generation, and reporting) locally. This monolithic, local-first design works for a prototype but will need refinement to meet scalability and cloud deployment goals. Key architectural improvements to consider:
    • Encapsulation of External Services: Currently, the Google Earth Engine (GEE) integration is scattered in the code (for example, calling ee.Image(...) and getThumbURL() directly in the ingestion module ). It would be wise to wrap such calls in a dedicated data provider class or module. For instance, a class EarthEngineClient could handle authentication and provide high-level methods like fetch_ndvi_time_series(region, start, end) or export_composite_images(...). This abstraction would make it easier to:
      a) Swap out or extend data sources – e.g. use a different API or a local dataset without altering analysis code,
      b) Inject dependencies for testing – a fake data provider could simulate GEE responses for unit tests,
      c) Centralize error handling and retries for external calls.
      More generally, define clear interfaces for I/O and computation as planned in the roadmap (“Modular interfaces: abstract I/O & compute backends” ). This will decouple Earth Engine logic from the core analysis logic.
    • Cloud-Friendly I/O and Config: To migrate easily to cloud, the system should minimize assumptions about local disk and interactive use. Right now, file paths like output directories (e.g. "output/ndvi_timeseries.csv") are hard-coded or passed via CLI arguments. In a cloud deployment, it might be better to parameterize storage locations (env vars or config files for bucket names, etc.) or use a storage-agnostic interface (like Python’s fsspec, which can handle local or cloud paths uniformly). Introducing a configuration system (using a YAML/JSON config or environment variables via a library like python-dotenv or pydantic settings) would allow the code to adapt to different environments (local vs cloud) without code changes. For example, a config could specify whether to use local disk or upload outputs to S3/Google Cloud Storage. Right now, such config is missing – integrating one will improve flexibility.
    • Stateless, Reentrant Design: Cloud execution (e.g. on AWS Lambda, Kubernetes jobs, Airflow tasks) often demands statelessness. Each run should not rely on global states persisting in memory between runs. The current code mostly adheres to this (each command reads inputs, writes outputs, and ends). However, there are potential global states to watch: for example, Earth Engine’s authentication and initialization is global (the code likely calls ee.Initialize() once and then uses the ee module globally). In a multi-user or multi-session environment, a global singleton might be fine, but it should be re-initialized when needed. It might be worthwhile to explicitly manage Earth Engine credentials (for example, allow providing a service account key or use OAuth tokens in an env variable) to avoid interactive prompts in a cloud context. Similarly, any use of global variables (none are obvious in the snippets beyond logger or ee) should be minimized or guarded by initialization checks.
    • Workflow Orchestration: The CLI pipeline report command currently sequences many steps within one process . In a cloud architecture, you might separate these steps into distinct tasks or microservices (for scalability and failure isolation). For example, one service/job could handle data ingestion (potentially heavy on I/O), another analysis (CPU-bound), another visualization (could be memory/CPU heavy for plotting). Using a workflow manager (Airflow/Prefect as mentioned in roadmap ) would allow these steps to be orchestrated with retries and parallelism. Even if not implementing a full Airflow pipeline yet, structuring the code in a way that each step can run independently is useful. The current pipeline function is somewhat linear and synchronous. A refactor could split it so that, for instance, the time series generation, the decomposition, and the report generation are exposed as separate functions or CLI subcommands (which they already are, to some extent) that could be called in sequence by an external orchestrator. This will make it easier later to, say, run the “download” step on a worker node close to the data (e.g. in Google Cloud), then pass results to an analysis container.
    • Scalability Considerations: As an MVP, the code may not yet handle large regions or long time ranges efficiently. Looking ahead, the architecture should allow scaling out. This might involve using distributed computing for large jobs (perhaps integrating Dask for large data in memory, or splitting work by region/time and parallelizing). The design could incorporate hooks for parallel processing – for example, if processing multiple regions, spawn threads or processes for each (ensuring Earth Engine’s rate limits are respected). Another consideration is memory usage: reading a large CSV into pandas can be memory-intensive; moving to chunked data (discussed under performance) or databases might be needed. The architecture should be ready to incorporate such changes without a total rewrite. Keeping modules loosely coupled and using interfaces (abstract base classes or at least well-defined function APIs) will ease swapping in more scalable implementations when needed.
    • Packaging for Deployment: Currently, the app can be installed with pip, and there’s a Dockerfile present (we haven’t inspected it, but it likely defines the environment). To deploy in cloud, ensure the Docker image is lightweight and secure (no hard-coded secrets, minimal base image). If the aim is to eventually release on PyPI, the architecture might also consider plugin support – for example, if external contributors want to add new “analytics” modules or datasets, a plugin system (using entry points) could allow that without modifying core. This is not essential now, but a nice extensibility point for the future.
    • Monitoring and Observability: As part of architecture, especially for cloud, consider how the system will be monitored. Adding structured logging and possibly metrics (e.g. how many regions processed, how long each step takes) will pay off. While not strictly architecture, it affects design choices (e.g. using a standard logging approach, perhaps emitting logs in JSON for easy parsing by cloud log systems). Likewise, if errors occur, how will they be surfaced? Thinking about these now can influence how functions return results or error codes (for example, returning status objects rather than just exiting on failure).

In summary, the architectural improvements revolve around making the system modular, configurable, and cloud-ready. Right now it’s a single Python program running sequentially on local data. The next iteration should introduce clear separations (external data vs internal logic, config vs code, independent modules), and prepare the codebase to scale (both in data size and in deployment context). These changes will facilitate the eventual Phase 2 goals of cloud deployment and more complex analytics.

4. Command-Line Interface (CLI) Assessment and Enhancements

The CLI is the primary user interface in this MVP, and it generally follows good practice by using the Click library (inferred from the @click.option decorators in the code) to define commands and options. The CLI currently provides commands like verdesat download, verdesat analyze, verdesat forecast, and verdesat prepare, as noted in the README quickstart . There is also a higher-level grouped command verdesat pipeline report for running an end-to-end workflow. Below we evaluate the CLI’s usability, flexibility, and extensibility, and propose improvements:
    • Discoverability and Help: With Click, each command likely has help text for its options (we see help strings in the code, e.g. “help=‘Temporal aggregation: D,M,Y’” for --agg ). Running verdesat --help should list available commands, and verdesat download --help would detail that command’s options. This is good. To further improve, ensure the descriptions are clear and complete. For instance, the download command might be more intuitively named timeseries or ndvi-download (to distinguish it from downloading raw imagery). In fact, the code seems to have a command function named timeseries behind the scenes in the CLI (the pipeline code commented an invocation of timeseries command ). If both download and timeseries exist or were renamed, that could confuse users. It should be clarified (maybe download was an early name now replaced by timeseries). Consistency in naming and documentation will help users understand the CLI.
    • Flexibility of Commands: The CLI covers basic scenarios, but making it more flexible can improve user experience. Suggestions:
        ◦ Allow multiple input formats: The prepare <input_dir> command suggests converting KML/KMZ or shapefiles to GeoJSON. It would be nice if verdesat download accepted either a GeoJSON file or directly a shapefile and performed the conversion internally by calling prepare. Currently, the user must run prepare separately to get a GeoJSON. Integrating this (or auto-detecting file type by extension) would streamline the workflow.
        ◦ Output options: Many commands default to certain output filenames or directories. For example, download likely outputs to a hardcoded CSV path (as seen in pipeline using output/ndvi_timeseries.csv). It would be better to let users specify an output path for data (with a default). This seems to exist (--output option is present for timeseries in the code) , but ensure every command that writes files has a user-configurable output path or directory.
        ◦ Parameterization: Extend options to cover more use cases. For example, the download/timeseries command might by default compute NDVI; but a user may want another index (EVI, etc.) or a different satellite collection. Currently, the collection/index might be fixed in code or partly exposed (e.g. there’s an --index option mentioned in code). Ensuring the CLI allows selecting the satellite dataset (Sentinel-2, Landsat, etc.), the index or bands, the spatial and temporal resolution, etc., will make it more powerful. These could be optional parameters with sensible defaults.
    • Usability Improvements: Some small enhancements for usability:
        ◦ Configuration file support: If a user is repeatedly running with the same parameters (say, same AOI and date range), a config file or profile system could avoid long CLI arguments each time. Click doesn’t natively support config files, but one can implement reading defaults from a file. Not critical for MVP, but a nice-to-have.
        ◦ Interactive prompts: For required options like --geojson, if not provided, the CLI could prompt the user to input a path. Click can do this (with prompt=True in option). This might be more friendly in interactive use. Similarly, for Earth Engine authentication, a CLI command to guide through authentication (open browser, etc.) would help new users (though the Earth Engine API has its own auth flow).
        ◦ Progress feedback: Long-running commands should inform the user of progress. Using a simple progress indicator or periodic log (via click.echo or logger) would prevent the user from thinking the tool hung. For example, if downloading a time series that involves many region calls, printing “Processing month X of Y…” or a progress bar is helpful.
    • Extensibility and Organization: As more commands are added (the commit history shows commands like chips, animate, plot being added for advanced features), the CLI module could get large. Right now, core/cli.py defines multiple commands and subcommands in one file . It might become cleaner to split the CLI definition by concern. Click supports grouping commands via subcommands or group classes. For example, you could have verdesat data <...> for data ingestion commands, verdesat analysis <...> for analysis commands, etc. In code, this might mean a package like core/commands/ with separate modules for each group. This way, adding a new command (say a new ML model) is as simple as adding a new module without risking breaking the monolithic cli.py. The current setup already introduces a pipeline group , which is a good start for grouping high-level workflows. Further grouping could be done (though not strictly necessary until commands grow in count).
    • CLI Interactions and Composition: The pipeline command demonstrates composing commands by invoking them programmatically (using ctx.invoke) . This is a clever use of Click to avoid rewriting logic. However, many of those invocations are currently commented out – perhaps due to incomplete implementation of some commands. In principle, this pattern is good for reuse, but it should be used carefully to not hide errors (since each invoked command might sys.exit on failure, which could interrupt the pipeline in an uncontrolled way). It might be safer to call the underlying Python functions directly instead of full CLI commands when chaining internally, to handle return values/exceptions in code. An improvement would be to separate core logic from CLI functions: e.g., have a function generate_timeseries(geojson, ...) in a module that both the CLI command and the pipeline can call. This avoids the indirection of Click context and makes the core functionality callable from other interfaces (like if a future web API wants to trigger the same actions). In summary, the CLI should remain a thin layer that parses arguments and calls business logic functions, rather than those functions living inside the CLI layer.
    • Error Messages and Exit Codes: Ensure the CLI yields clear messages. For example, if a user enters an invalid date format, the code might throw an error – it would be better if the CLI catches it and prints “Error: invalid date format, please use YYYY-MM-DD”. Click can capture exceptions and print them nicely if configured. Also, set appropriate exit codes (non-zero on failures) so that in scripting or pipeline usage, errors are detectable. Given this is an MVP, these details might not be in place yet, but they are important for a polished CLI experience.

Overall, the CLI design is solid for a start: it uses a standard library, provides multiple subcommands, and demonstrates an understanding of grouping and reuse. The improvements above focus on making it more user-friendly (through flexibility and feedback) and maintainable as it grows (through better organization and decoupling from logic). By implementing these, the CLI will be a robust interface suitable for both interactive use and integration into larger workflows.

5. Module Interactions and Potential Refactoring

With multiple modules handling different tasks, it’s important to ensure they interact cleanly, without duplicating functionality or being too tightly coupled. After reviewing the structure and code snippets, here are observations and suggestions regarding module interactions:
    • Duplication of Functionality: We should eliminate any overlapping code across modules. One area to check is data handling across ingestion and analytics. For instance, the ingestion module returns raw data (CSV or images) that the analytics module then reads. If both modules implement similar routines (e.g. both need to read a CSV file, both need to parse dates), consider centralizing that. A utility function in core/utils.py for reading a time series CSV into a DataFrame could be used by analytics.aggregate and analytics.decompose rather than each doing file I/O separately. Similarly, the logic to buffer geometries (adding a buffer distance around polygons) is introduced in the ingestion code . If other parts of the code need geometry operations, leveraging a common library (shapely or geopandas) and possibly writing a helper in core/geo.py would prevent reimplementing those steps elsewhere. So far, it looks like the team is careful (they likely use geopandas in prepare command to handle projections, etc.), but vigilance against creeping duplication is needed as features are added.
    • Improper Coupling: There are a few instances where modules are somewhat coupled via global context or assumptions. For example, the visualization/report module depends on outputs from analytics and ingestion: it expects a set of images (decomposition plots, gallery of chips, etc.) and a stats table . Currently, the pipeline assembles these pieces and passes them to report. This is okay, but in the long run consider defining clearer data contracts between modules. One way is using data classes or objects – e.g., a TimeSeriesResult object from analytics that contains the filled time series and stats, or an ImageCollectionResult from ingestion with references to image files or arrays. Passing objects instead of file paths reduces the risk of misusing an output in the wrong way. In the MVP, files and file paths are the interface (e.g., ctx.invoke(decompose, input_csv=..., output_dir=...) writes images to a known folder which report then reads). This works but is somewhat fragile: it relies on a specific folder structure and naming convention. A more abstract approach would have decompose() return a Python object (or at least return the list of generated file paths), so the next function doesn’t have to glob the filesystem or assume names. Minimizing inter-module hidden dependencies (like a hardcoded expectation that chips images are in chips_monthly/ directory) will make the system more maintainable.
    • Use of Global State vs Parameter Passing: The design should avoid using global variables to share data between modules. Instead, pass what is needed explicitly. For example, if the modeling module needs to know which index was analyzed (NDVI or something else), that should be passed in, not inferred from a global config that ingestion set. In the current code, one potential global state is the Earth Engine API (ee module) which is imported and used in multiple places. That’s acceptable since it’s a third-party singleton, but anything the project defines (like a configuration for current AOI, or the loaded GeoJSON) should be passed as function arguments. We didn’t see evidence of problematic globals in the code, but it’s a point to watch as features grow. The mention of global objects in the prompt likely refers to making sure things like a GeoJSON dataset isn’t read once and then stored globally for others to use – rather, each function should get the data it needs (or references to it).
    • Abstraction Barriers: Look for opportunities to create higher-level abstractions that simplify interactions. One such opportunity is time series data handling. Right now, the flow is: ingestion yields a CSV of time-indexed values, analytics functions like aggregate or fill operate on CSV (likely using pandas internally), then visualization reads the CSV to plot. This could be abstracted by a class like TimeSeriesData that wraps a pandas DataFrame and provides methods like .aggregate(freq) or .fill_gaps() and .to_csv(). Then modules could pass around TimeSeriesData objects instead of raw file paths. This would reduce the coupling to CSV files and also encapsulate the data transformations. Similarly, an object representing a set of images could have methods to create a gallery or an animated GIF. Such abstractions can prevent a lot of low-level file handling from leaking across modules.
    • Inter-module Communication: If keeping the current approach of using the file system as the interface (one module writes files, another reads them), at least standardize those outputs. For example, ensure consistent file naming and formats: the project already chose CSV for time series and PNG for images, which is fine. But ensure that, say, the CSV has a known schema (column names, etc.) that analytics and viz modules both expect. The snippet shows that the time series CSV likely has a column “mean_ndvi” or similar . This column name is used in plotting (passed as index_col="mean_ndvi" to the plot function). Such implicit knowledge should be documented or enforced. Perhaps define a constant for the expected column name, used by both generator and consumer. The risk of not doing so is if one side changes (e.g., decide to output median NDVI too), the other side might break. Writing tests for these interactions (though tests are deferred) will also help catch coupling issues.
    • Removing Dead/Obsolete Code: During development, some functions may become superseded. For instance, originally a separate download and analyze command existed, and now there’s an integrated pipeline report that wraps them. There are large blocks in the pipeline function commented out (for steps 1 and 4 and 5, etc.) . These likely correspond to calling other commands (timeseries, chips) that might have been under development. It’s important to periodically clean up these to avoid confusion. If those steps are not ready, perhaps leave them as optional (with a flag) or remove from the pipeline until they work. Duplicated logic can also sneak in this way – e.g., the snippet showed two different implementations of determining min/max stretch for images, one of which was removed in a refactor . After refactoring, ensure only one approach is present. Regular refactoring and pruning of such duplicates or commented code will keep the codebase coherent.
    • Cross-Package Imports: One subtle point: ensure that lower-level packages do not import higher-level ones, to avoid circular dependencies. For example, the ingestion module should ideally not import from analytics. Each should possibly import from core utilities or standard libs, but not each other (except via clearly defined interfaces). Based on the structure, it seems core/cli imports and invokes everything, which is fine. Just be cautious that, say, an analytics function doesn’t directly call an ingestion function; it should receive data via parameters. Maintaining this separation will allow independent development and even packaging of submodules in the future (if, for instance, the team decided to split some parts into separate libraries).

In essence, the goal is to have modules interact in a clear, minimal interface fashion – passing data or calling well-defined functions – rather than implicitly via files or global variables. By introducing simple data structures for common data (GeoJSON features, time series arrays, image collections) and standardizing how they are passed around, the system will avoid tight coupling and duplicate code. The current MVP is not overly complex, so these improvements are preventive measures as the code grows. Adopting them early will save effort down the road.

6. Object-Oriented Design and Design Patterns

The current codebase appears to be largely procedural/functional, which is understandable for a small MVP. However, as functionality expands, incorporating more Object-Oriented Programming (OOP) principles could improve organization, reuse, and clarity. Here are some observations and suggestions in that vein:
    • Data Encapsulation with Classes: Certain entities in this domain lend themselves to classes. For example, consider introducing a class for a GeoSpatial Asset or Region. This class could encapsulate a geometry (from a GeoJSON or shapefile) and provide methods to perform common operations (e.g. reprojecting, buffering, computing area). In the MVP, the prepare command likely reads various spatial files and outputs a unified GeoJSON – this logic could be part of a class RegionSet or AOICollection that knows how to ingest different formats and output standardized GeoJSON. Similarly, a class TimeSeries could encapsulate the NDVI time series for a region (with attributes like dates and mean values, and methods for interpolation, aggregation, etc.). In the current code, these operations are spread across functions (aggregate, fill_gaps, etc. likely operate on pandas DataFrames or CSVs). Wrapping them in a class would bundle the data with the methods that act on it, following encapsulation. For instance, one could call ts = TimeSeries.from_csv("timeseries.csv"), then ts.aggregate("M").fill_gaps().decompose() – a fluent interface that hides file I/O and uses internal state. This would reduce passing around filenames and repeatedly reading/writing to disk between steps.
    • Reducing Global State: As mentioned, global variables should be minimized. If Earth Engine requires a global initialization (ee.Initialize()), one pattern is to wrap it in a singleton class. For example, an EEManager class could ensure initialization happens and possibly manage rate limiting or retries for API calls. That way, instead of calling ee.Image everywhere, you call EEManager.get_image(...), and that method internally checks if EE is initialized (and does it once globally). This is a subtle use of a global (EEManager would itself be globally used), but it provides a layer to inject things like a different project ID or service account if needed, rather than scattering those across modules. It appears the code already added an option --ee-project to override the Earth Engine project ; this could be stored in such a manager object instead of threaded through multiple function parameters.
    • Applying Design Patterns: Depending on future needs, certain design patterns can be beneficial:
        ◦ Factory Pattern: If supporting multiple data backends (Earth Engine vs local files vs a STAC API), use a factory to create the appropriate data retrieval object. For example, a DataSourceFactory could return an NDVIDownloader object of the correct subclass based on a config (one subclass for EarthEngine, another for reading from local GeoTIFF time series). This ties into the earlier suggestion of abstracting I/O; a factory makes it easy to choose the implementation at runtime.
        ◦ Strategy Pattern: For analysis steps like forecasting, one could define an interface for a forecasting strategy. The forecast command could then use a selected strategy (Prophet, or an LSTM model, etc.) without changing its code. Strategies can be configured via CLI (e.g. --method prophet vs --method lstm). Internally, a Forecaster class with subclasses ProphetForecaster, LSTMForecaster can implement a common method (e.g. .forecast(ts)).
        ◦ Builder or Pipeline Pattern: The way the pipeline_report is structured (stepwise assembly of outputs) suggests a Pipeline pattern could be formalized. Perhaps a ReportBuilder class could accumulate the pieces required for the final report. It could have methods like .with_time_series(ts), .with_decomposition(images), .with_gallery(gif_dir) and then .build_report(output_path). This would make the assembly of the report more robust than a long function with local variables. It also separates the concern of “how to build a report” from the CLI orchestrator.
        ◦ Observer Pattern or Callbacks: If the system will incorporate events (for example, updating a progress bar or triggering a next step when one completes), an observer pattern might be useful. This is more applicable in a UI or asynchronous context (perhaps not needed now, but if integrating with a web dashboard, it could be relevant).
    • Using Dataclasses: Python’s dataclass is a lightweight way to create classes for data holding. For instance, a RegionStats dataclass could hold the statistics for a region (min, max NDVI, trend, etc.), making it easy to pass that around rather than multiple separate values. The code already computes a stats summary table for the report – this might be represented as a list of RegionStats objects rather than a raw list of dictionaries or pandas DataFrame, depending on use case. Dataclasses automatically generate init and repr, which is convenient for such uses.
    • Efficient Data Formats (Zarr, etc.): This is slightly separate from pure OOP, but it’s mentioned: using Zarr or similar formats may be beneficial for performance and data handling. If adopting Zarr for, say, storing time series or image collections, one could also introduce classes to interface with those. For example, a SatelliteDataCube class might manage a Zarr store of imagery (time x lat x lon) and provide methods to retrieve a time series for a given region (essentially doing what Earth Engine does, but on local/cloud data). Zarr is well-suited for cloud because it allows partial reads of large arrays. Studies have shown that using Zarr for time series queries can reduce query time from minutes to seconds by reading only needed chunks . Integrating such capability might be beyond MVP scope, but the design should not preclude it. If the current design is too tied to CSVs and individual images, migrating to a chunked multidimensional format could be hard. But if you introduce abstraction (like a TimeSeriesData class that could internally switch from CSV/Pandas to Zarr/Xarray later), then you can hide the storage change behind the class interface. In short, plan for efficient formats by abstracting away direct file handling in the code logic. This isn’t OOP for its own sake, but using objects to represent data can ease transitions to new storage backends.
    • Avoiding Over-Engineering: While adding classes and patterns, be mindful not to complicate the simple use cases. The current functional approach is easy to follow for a small number of steps. A pragmatic path could be to identify a few key classes that provide clear value (like encapsulating Earth Engine calls or representing a results dataset) and implement those first. For example, a VerdeSatSession class could be introduced to hold configuration (like project IDs, paths, etc.) and maybe provide methods to run common pipelines. That would bundle configuration and execution context, rather than passing many options around. The goal is to make the code more readable and modular, not to introduce an unnecessary OO layer. Given the MVP nature, the team may have avoided classes to keep it straightforward. But as complexity grows, thoughtful introduction of OOP can actually simplify usage (especially if the CLI can be a thin wrapper around methods of these classes).

In conclusion, employing OOP principles where appropriate can help structure the VerdeSat application for extension and clarity. Encapsulating data and functionality into classes (GeoJSON assets, time series data, Earth Engine client, etc.) will reduce repetition and make the code’s intent clearer. Design patterns like factory or strategy can prepare the system for future variability (multiple data sources or algorithms). And efficient data management can be achieved by pairing format choices (like Zarr) with classes that abstract their details. The key is to use these techniques to manage complexity, which will pay off as the project scales beyond the MVP.

7. Avoiding Reinventing the Wheel

A crucial aspect of any software project is leveraging existing libraries and tools instead of writing everything from scratch. VerdeSat’s developers have wisely included many robust libraries in their dependency list (e.g. GeoPandas for GIS, SciKit-Learn for ML, Prophet for forecasting, etc. ), suggesting an intent to build on well-tested foundations. We should ensure the implementation indeed uses these libraries to their full potential and doesn’t duplicate functionality that they provide. Here are areas to watch:
    • Geospatial Data Handling: The prepare command likely involves reading spatial files and converting to GeoJSON. Instead of manually parsing KML or Shapefiles, the code should use GeoPandas/Fiona (since geopandas is a dependency ) to read these formats. GeoPandas can unify different file types into a GeoDataFrame, and then easily export to GeoJSON. If the code is currently doing things like reading JSON text or manually manipulating coordinates, that would be a reinvention. It’s presumed they are using the library (reinventing a projection transformation or coordinate conversion would be unnecessary given pyproj and shapely are available). So the recommendation is: continue to rely on these libraries for any geospatial math (area calc, buffering, etc.) rather than coding it manually. The commit logs indicate features like “add percentage-buffer” and “cloud masking” were added – hopefully those use known techniques (e.g. using Earth Engine’s masks or rasterio for buffering raster edges) rather than custom code. If any custom spatial logic was implemented, consider if a library exists (e.g. rasterio can apply masks and rioxarray can buffer or pad raster datasets).
    • Time Series Analysis: For analyzing and filling time series, Python’s ecosystem is rich. Pandas should be used for resampling and interpolation (it likely is, given the nature of the task). If the fill_gaps function is written manually (e.g. looping over dates to fill missing months), that is reinventing what pandas.DataFrame.asfreq() or interpolate() can do. Similarly, seasonal decomposition is available via StatsModels (seasonal_decompose), and forecasting via Prophet or scikit-learn for simpler regressions. The code should not attempt to implement complex statistical methods from scratch – instead configure and use these libraries. Given Prophet is listed as a dependency , the forecast command likely uses it rather than a homegrown forecasting algorithm. One thing to check is the trend calculation for NDVI: if they just do a linear regression or a Sen’s slope, there are libraries for that (even numpy/polyfit or SciPy). Reinventing regression would not be needed. So far it seems they avoid that by using standard tools.
    • Image Processing and Formats: The commit “feat: convert to COG” suggests they generate Cloud-Optimized GeoTIFFs . Converting a regular TIFF to COG can be done via gdal_translate or rasterio. If they wrote custom code to manipulate TIFF bytes, that would be overkill – hopefully they are calling out to GDAL (maybe via subprocess or using rasterio’s COG driver). Likewise, generating animated GIFs (“add gif generation” in commits ) can be done with imageio or PIL. It’s fine if they did it manually by assembling images, but using imageio to write a GIF is one-liner. Ensure that these tasks use tried-and-tested libraries to handle nuances (palette, frame timing, etc.). The mention of “fix missing frames in GIFs” implies they encountered issues that likely a robust library would abstract away.
    • Web/HTML Generation: The project uses Jinja2 templates for HTML report generation , which is good (no need to manually concatenate HTML strings). For any interactive components, they might consider libraries instead of custom JS – but since they haven’t gone deep into web (besides an HTML file), that’s fine. If later they want interactive plots, they should consider using Plotly (which they listed as a dependency ) rather than writing JavaScript from scratch. Ensuring the report leverages Plotly for interactive charts (and maybe a light-weight JS library for image sliders if needed) will prevent reinventing front-end logic.
    • Machine Learning & Stats: In future expansions (land cover classification, etc.), it will be important to use frameworks like scikit-learn, PyTorch, TensorFlow as appropriate, rather than coding algorithms manually. Given these are already dependencies, the expectation is they will. For example, if doing an unsupervised clustering on imagery, use scikit-learn’s clustering; if training a deep model, use PyTorch’s extensive API. There’s no sign of them reinventing such wheels in the current code (since modeling is mostly stub), but it’s a note for going forward.
    • Existing Earth Engine Utilities: There are some community tools for Earth Engine in Python, such as geemap (which provides a high-level interface to Earth Engine and can handle downloads, conversions to xarray, etc.). If the project finds itself writing a lot of boilerplate for Earth Engine (like constructing image collections, mapping, exporting), it could look into these libraries. However, adding too many abstractions can also be a double-edged sword if it reduces control. So far it seems VerdeSat interacts with Earth Engine via direct API calls (which is fine). Just be aware of not duplicating what Earth Engine already offers – e.g., Earth Engine can compute composites and NDVI internally; the code should call those server-side functions instead of downloading raw bands and computing NDVI in Python (which would be much slower). It appears they do NDVI server-side (since they request images or thumbnails presumably already styled for NDVI), which is good. In summary: use Earth Engine’s capabilities (cloud computing, built-in datasets) as much as possible instead of treating it just as a data source to post-process entirely on the local side.
    • Standard Libraries: Another small area – if any text processing or file handling is needed, Python’s stdlib often suffices. For example, no need to manually parse CSV with custom code when pandas or Python’s csv module exist. Ensure encoding issues, etc., are handled by using these libraries. There’s no direct evidence of a problem here, but it’s a general admonition not to roll your own parser or utilities that can be imported.

In conclusion, VerdeSat’s developers have set themselves up well by choosing a rich set of libraries. The key now is to lean on those libraries for all heavy lifting. Any code that starts to grow complex (e.g. math, parsing, image manipulation) should trigger a check: “Is there already a tested library or function that does this?”. By avoiding wheel reinvention, the project gains robustness and can focus on the unique logic that ties these components together.

8. Performance Optimization Strategies

Optimizing performance is crucial as the project scales in data volume and complexity. The MVP likely runs fine for small regions and time spans, but bottlenecks will appear with larger use cases. Here are several strategies to improve runtime performance, with analysis of the current approach:
    • Leverage Server-Side Computation (Earth Engine): One of the biggest advantages of Google Earth Engine is its ability to perform computations on Google’s servers near the data. The current implementation should ensure it’s utilizing this capability. For example, when retrieving NDVI time series, it’s more efficient to use Earth Engine’s reduceRegion or reduceRegions to compute mean NDVI per region per date in Earth Engine, and then bring back only the aggregated results. If instead the code were retrieving full-resolution images and computing means in Python, that would be extremely slow and unnecessary. The code snippets (use of getThumbURL for PNGs and getDownloadURL for GeoTIFFs) suggest they are pulling already-rendered images , which is okay for visualization but not for numeric time series. Ideally, numeric results (like mean NDVI values) should be fetched as JSON or CSV via Earth Engine’s getInfo() or export to Drive, rather than computing on the Python side. In short, push as much computation to Earth Engine as possible – it will run on parallel infrastructure. Only download what you need. This strategy reduces both time and bandwidth.
    • Batching and Parallelism: The code currently loops over images when exporting image chips one by one . This sequential approach might be slow if count (number of images) is large. Earth Engine can often handle lists of images in parallel via its API if used cleverly, but the Python client itself is single-threaded in calls. If local processing is the bottleneck (e.g., making many HTTP requests to fetch images or data), consider employing parallelism on the client side. Python’s concurrent.futures.ThreadPoolExecutor could be used to fire off multiple requests for thumbnails simultaneously (since I/O bound). However, caution: Earth Engine might rate-limit or you might saturate the network, so test carefully. Another approach is to use Earth Engine’s asynchronous batch export capabilities: for example, trigger an Earth Engine export task for all images and periodically check status. That offloads the waiting to the server side. In summary, try to batch requests (fetching multiple regions or dates in one call if possible) and use parallel threads for I/O when appropriate.
    • Efficient Data Formats and In-Memory Ops: The suggestion to use Zarr or Parquet for storing intermediate data isn’t just for convenience; it’s a performance consideration. CSV is human-readable but slow to parse for large datasets and tends to be larger on disk than binary formats. If the time series data for many regions or many timestamps grows, switching to a columnar binary format (Parquet) or a chunked array (Zarr) will speed up read/write. For example, reading a large CSV into pandas can take significantly longer and more memory than reading the same data from Parquet (which is compressed and optimized). Parquet also would allow querying subsets (with tools like pandas.read_parquet(columns=[...], filters=[...])). As noted earlier, Zarr enables partial reads of big arrays, which is great for time series or image data cubes . If, for instance, a future requirement is to analyze pixel-level time series over a 1000x1000 region, storing that as a Zarr array (with chunks in time) would allow reading one pixel’s series extremely fast, whereas storing each pixel’s series in CSV or as separate files would be slow. Even for the current aggregated data, if there are many regions, a single Parquet with a region ID column would let you filter by region quickly. Converting to these formats is an optimization that can be introduced when needed; the architecture should not prevent it.
    • Vectorization and Algorithmic Efficiency: Ensure that computational steps in Python are using vectorized operations rather than Python loops. The use of pandas and numpy is likely, which is good. For example, filling gaps in a time series should be done with pandas .reindex() and .interpolate() rather than a manual loop over timestamps. Similarly, computing statistics (min, max, percentile) on arrays should use numpy’s efficient C-backed operations. The commits about “fix the table” and adding a stats summary suggest they calculate stats, hopefully using pandas describe or groupby rather than manual iteration. Always check if a library function can do an operation in compiled code. A telling example: the code snippet for image export originally computed min/max for visualization in Python for each image , but this was refactored to compute defaults once and reuse – that’s a minor efficiency gain, but a good practice (don’t repeat work if not needed). Look for similar opportunities: e.g., if the same calculation is done for each region, see if it can be done in one go for all regions (Earth Engine can do all regions together by mapping over a FeatureCollection).
    • Memory Management: For local operations, be mindful of memory. If reading large datasets into memory, consider if streaming or chunking is possible. For instance, if you had to process a very long time series CSV, reading it in chunks and processing incrementally (or using dask.dataframe for out-of-core computation) might help. In the MVP context, this probably isn’t an issue yet, but planning for it matters. Also, after certain steps, if data is no longer needed, explicitly delete large objects or use generators to pipeline data. Python’s garbage collector will handle most, but in long pipelines, sometimes a large DataFrame lingering can hurt performance later due to memory pressure.
    • Use of Caching (Memoization): When implementing the caching layer (planned in Phase 2), a simple but effective approach is to cache results of expensive computations. For example, if the same region and date range is requested twice, you shouldn’t recompute it the second time. Even within one run, if a certain intermediate result can be reused by multiple steps, store it. The pipeline code currently writes intermediate CSVs and reads them later, which is a form of caching on disk between steps. This could be optimized to in-memory passing of objects (to avoid disk I/O), or if persisted disk caching is desired across runs, a mechanism to check “have I already computed NDVI for this GeoJSON and date range?” would save time. Implementing caching can range from using Python’s functools.lru_cache for memory caching of function calls to more sophisticated persistent caches (like a local database or file hash-based storage). Given the roadmap’s mention of caching, this is on the radar – it will significantly improve performance for repeated runs or incremental updates.
    • Parallelizing Independent Work: Some tasks are embarrassingly parallel. For example, if you have 10 regions to analyze, you could process them in parallel since they don’t affect each other (assuming Earth Engine can handle multiple concurrent requests or using separate quotas). In a cloud scenario, spinning up multiple workers for different regions or time chunks could linearly speed it up. The architecture could support this by making sure that the code can operate on subsets easily (i.e. avoid monolithic functions that assume a single-thread context). If not done already, designing functions to process one region, one time-series, etc., can allow an external orchestrator or threading to run multiple simultaneously. Using joblib or multiprocessing in Python could also help for local multi-core usage, particularly for CPU-bound parts like model training or heavy numpy operations.
    • Profile and Identify Hot Spots: As a practical tip, once the code is more feature-complete, use profiling tools (cProfile, line_profiler) to find where the time is spent. It might turn out that, say, generating plots is slow (maybe using matplotlib could be slow for many points; using Plotly (with WebGL) might be faster for huge datasets). Or maybe the bottleneck is waiting on Earth Engine. In that case, more aggressive parallel request logic or optimizing the EE script (reducing server load) would be the focus. Since the MVP is not yet at scale, now is the time to ensure the design doesn’t preclude optimizations. Early testing on moderately large inputs can guide which part to optimize first.

In summary, to improve runtime performance: do less work, do work in parallel, and do work in the right place. Use Earth Engine for what it’s good at (big geo computations), use efficient formats and libraries for local computations, and restructure the code to avoid unnecessary repetition. Adopting these strategies will ensure that as VerdeSat moves from a local MVP to cloud-scale, it will handle large data volumes and user loads efficiently.

Sources:
    • VerdeSat Repository README and Code Snippets (Project structure, best practices, and code examples)
    • Commit History of VerdeSat (selected commits showing code changes and TODOs, e.g., pipeline implementation and image export) 
    • Earthmover Tech Blog – What is Zarr? (discussion on using Zarr with Xarray for efficient large data access) 
    • AWS Public Sector Blog – Using Zarr on S3 for geospatial data (notes performance gains for time-series queries with Zarr) 
VerdeSat Refactoring and Expansion Roadmap

VerdeSat is a minimal Earth observation toolkit focused on sustainability, currently in an MVP state. The following roadmap and sprint plan will guide a two-person team through a comprehensive refactor and feature expansion. The primary goals are to improve modularity, adopt clean object-oriented design, enhance system architecture and performance, and make the CLI more user-friendly. Testing, documentation, and CI/CD will be addressed after the core refactoring. The plan also incorporates insights from the Earth observation open-source community to avoid reinventing the wheel.

Roadmap Stages and Milestones

Stage 1: 
Core Refactoring & Architecture Modernization

Refactor the codebase to enforce a modular, object-oriented architecture. Leverage the existing repository structure (with dedicated folders for core, ingestion, analytics, modeling, etc. ) to separate concerns and improve maintainability. Key improvements include abstracting I/O and computation layers, so that data sources (e.g. local files, Google Earth Engine, future openEO backends) and processing engines can be swapped easily. This stage establishes a clean foundation for all other enhancements.
    • Milestones: Completed reorganization of code into classes and modules (e.g. separate DataIngestion, Analysis, Modeling classes); abstract interfaces for data retrieval and computation backends (preparing for local vs cloud execution); core utilities (config, logging) centralized in the core/ module; all existing functionality still works after refactor, validated by basic tests.

Stage 2: 
CLI Usability Enhancement

Redesign the command-line interface for clarity and consistency. Currently, VerdeSat uses grouped subcommands (e.g. verdesat download timeseries, verdesat visualize plot), which will be reorganized for intuitive use. Possible changes include flattening trivial groupings or regrouping commands by domain (e.g. a top-level data command with subcommands like download, versus separate download and prepare commands). The CLI help and documentation will be improved for better guidance. Emphasis is on making common tasks easy to discover and run.
    • Milestones: New CLI command hierarchy implemented (with clear grouping or simplified commands as per the audit findings); updated help texts and usage examples; verification that all commands execute correctly with the refactored core logic; documentation (README/CLI help) reflects the new command structure.

Stage 3: 
Performance Optimization & Data Handling

Identify and address performance bottlenecks in data processing. This involves utilizing efficient libraries and patterns for geospatial data (e.g. using xarray/dask for large raster time series, vectorized operations in pandas/numpy). A caching layer will be introduced to avoid redundant data downloads and computations, improving repeat run times . The system architecture will be evaluated for concurrency opportunities (e.g. parallel downloads of tiles) and memory optimizations. This stage ensures the toolkit runs faster and can handle larger datasets smoothly.
    • Milestones: Caching mechanism implemented for satellite data (e.g. local cache of downloaded images or time-series results to reuse in future runs ); heavy computations (such as time series calculations or image processing) refactored to use optimized libraries (confirmed by performance tests showing speed improvements); capability to process daily or custom-timeframe composites added without performance degradation (leveraging chunking or parallelism as needed); no regression in results accuracy after optimization.

Stage 4: 
Data Ingestion & Analytics Expansion

Expand the range of data sources and analysis capabilities. On the data side, support will be added for daily or custom time-window composites, in addition to the current monthly NDVI (as outlined in the Phase 2 roadmap ). This means users can specify different temporal resolutions for satellite data compositing. In-situ data (e.g. CSV files of field measurements) will be ingestible and joinable with satellite data for analysis. We will also incorporate external tools where possible – for example, evaluating frameworks like EODAG for unified data search/download across providers – to avoid reinventing data access code. On the analytics side, new analysis features include advanced trend analysis (e.g. seasonal decomposition or change detection) and interactive visualization. A lightweight dashboard (using Dash or Streamlit) will be prototyped to display time-series and map visualizations, aligning with the roadmap goal of interactive analytics .
    • Milestones: Functionality to download and compose imagery at different time resolutions (e.g. daily composites) is implemented and exposed via CLI; support for reading in-situ observational CSV data and merging it with satellite time-series data; at least one interactive visualization demo (e.g. a Streamlit app in the webapp/ module) showing a sample analysis; new CLI commands or options for these features (e.g. verdesat data composite ..., verdesat analyze trends ..., or verdesat visualize dashboard) are in place and documented.

Stage 5: 
Modeling Enhancements

Introduce robust modeling capabilities for forecasting and classification as planned. This includes implementing time-series forecasting with both a machine learning approach (e.g. Facebook Prophet for univariate trends) and deep learning (e.g. an LSTM model for sequence data), in line with the Phase 2 plan . The modeling module will be structured with clean OOP design: e.g. a base Model interface with subclasses for each model type, enabling extension. Hyperparameter tuning will be integrated (using Optuna) to demonstrate automated model improvement . We will ensure models are decoupled from data ingestion through dependency injection (passing prepared data into models) as recommended in the best practices . This stage sets the groundwork for more complex modeling while keeping the code maintainable and testable.
    • Milestones: Two forecasting pipelines implemented (e.g. Prophet and a simple LSTM) and runnable on example data through the CLI (e.g. verdesat forecast --method prophet); hyperparameter tuning example integrated (e.g. an Optuna script or command to optimize a Prophet model’s parameters ); clear separation of model code from data access (models consume data inputs rather than calling I/O themselves, enabling future cloud-based data feeding); preliminary evaluation of model performance on sample datasets with results documented or plotted.

Stage 6: 
Code Quality, Testing & Documentation

Throughout the refactor and new feature implementation, maintain a strong focus on code quality. This involves adding type hints and docstrings to all public functions and classes (enforced via tools like mypy) and adhering to Python style guides with automated linters (black, flake8) . Comprehensive unit tests will be developed alongside features, aiming for ~80% coverage by the end of Stage 5 . By the conclusion of the refactoring sprints, the project should have a significantly improved README and usage documentation (though a full rewrite and tutorial can follow once features stabilize). Continuous integration (CI) will be set up to run tests and linters on each commit, preparing for future continuous deployment.
    • Milestones: Static typing checks pass (no mypy errors) on the codebase ; code passes linting/formatting checks (with a pre-commit hook configured for development) ; test suite covers major modules (target ≥80% coverage) and is integrated with a CI workflow (e.g. GitHub Actions) for automated testing; updated documentation (README and possibly a dedicated docs site) covering installation, new CLI usage, and examples.

Stage 7: 
Packaging & Future Deployment (Post-refactor)

(Planned after the initial two sprints) Prepare the project for broader use and future cloud deployment. As an open-source local-first tool, packaging the library for easy installation is important – this includes setting up versioning (semantic versioning with a CHANGELOG) and publishing to PyPI. CI/CD will be extended to handle package building and publishing when appropriate . In parallel, design considerations for cloud migration will be revisited: e.g. containerization with Docker/Helm and orchestrating tasks on cloud platforms . We will also explore aligning with openEO API standards to ensure interoperability – openEO provides a unified way to connect to various cloud-based Earth observation backends , and VerdeSat’s architecture (from Stage 1) will make it feasible to plug into such services. This stage is about transitioning from a refactored MVP into a production-ready, scalable service.
    • Milestones: PyPI package setup (project can be installed via pip); version bump and CHANGELOG entry for the refactored release; CI pipeline configured to run tests, then build and publish on new releases ; Dockerfile updated (if necessary) to reflect new architecture and tested for local deployment; a documented plan or prototype for running core processing in a cloud environment (could be as simple as a note on how to use openEO or STAC services in place of local data sources, demonstrating the flexibility of the new design ).

The above stages are prioritized in roughly the order given (refactor foundations first, then usability, performance, feature expansions, etc.). Testing and documentation improvements are woven throughout the process rather than left to the end. Packaging, CI/CD, and cloud considerations are acknowledged in design choices early on, but most implementation for those will occur after the core refactoring and feature addition sprints.

Sprint 1: Core Architecture & CLI (Week 1)

Sprint Focus: Laying the groundwork with a modular architecture and improved CLI, while maintaining existing functionality. We assume a one-week sprint (~40 hours of work for the 2-person team). Tasks are designed to be parallelizable between two developers where possible.
    • Audit & Design Planning (Day 1): Review the current MVP code structure and the recent audit findings. Identify areas of tight coupling or script-like structure that need refactoring into classes. Evaluate external resources for guidance – e.g. review the OpenEO API and Earth observation toolkits to inform our design. This research ensures we align with industry-best practices and reuse existing solutions where appropriate (for example, considering an API like openEO’s for future compatibility , or using an existing downloader library instead of custom code). Outcome: A brief design document or whiteboard outlining the new module/classes layout and CLI structure changes.
    • Refactor Core into Modules/Classes: Implement the planned restructuring of the codebase. Break up monolithic scripts into a clear module hierarchy as per the repo structure :
        ◦ Move generic utilities (config parsing, logging, file I/O) into verdesat/core/ (e.g. a ConfigManager class, Logger setup, utility functions).
        ◦ Refactor data-related logic into verdesat/ingestion/ (e.g. create a SatelliteDataIngestor class handling data download and preprocessing).
        ◦ Refactor analysis functions into verdesat/analytics/ (e.g. a class or set of functions for time-series analysis, currently used by analyze CLI).
        ◦ Ensure the new classes have clear interfaces (methods) and use initialization to manage state where appropriate (for example, a DataIngestor might be initialized with an API key or output directory).
        ◦ Implement interface/abstraction layers for key components: for instance, define an abstract base class or protocol for data retrieval so that one implementation can use Google Earth Engine now and another could use a different provider later (openEO, local files, etc.). This will make it easy to plug in new backends without altering higher-level code .
        ◦ Outcome: A cleaner project structure with core logic encapsulated in classes. Code is easier to navigate (each module focuses on a domain), and preparation is made for extending or swapping components.
    • CLI Command Reorganization: Redesign the CLI using a framework like Click or Typer (if not already in use). Incorporate the audit suggestions to improve command grouping and naming:
        ◦ Simplify where possible: If a command group has only one subcommand (e.g. verdesat download timeseries was the only download option), consider merging it (e.g. make verdesat download do the timeseries by default, with options for other types later) for a smoother UX.
        ◦ Introduce logical grouping where it helps clarity: for example, ensure that preparation steps (prepare) and data download functions are under a common theme (could be verdesat data ... group) or clearly distinct from analysis commands.
        ◦ Update the CLI help text and usage examples to reflect the new structure. Include descriptions for each command and subcommand so users understand their purpose.
        ◦ Verify that the example workflow (verdesat download ... -> verdesat analyze ... -> verdesat forecast) still works with the new CLI or update the workflow if commands changed.
        ◦ Outcome: A cleaner CLI interface that is user-friendly and consistent. Running verdesat --help should clearly list grouped commands (download/prepare, analyze/visualize, forecast, etc.) in an intuitive order.
    • Implement Logging & Config Improvements: As part of the refactor, ensure there is a unified approach to configuration and logging:
        ◦ Use a config file or object to manage parameters like default data directory, API credentials, etc., rather than scattering constants. Perhaps implement a verdesat.ini or use environment variables (with core module reading them).
        ◦ Integrate a logging framework (Python logging module) for debug/info messages instead of print statements. Provide a verbose mode flag in CLI to toggle debug logging.
        ◦ Make sure these utilities are initialized in a central place (for example, when the CLI starts, it loads config and sets up logging via the core module).
        ◦ Outcome: Consistent logging across the application (messages with timestamps/severity), and a single source of truth for configuration that can later be extended (e.g. if we add cloud configs).
    • Code Quality Pass (Linting & Typing): Begin enforcing code quality standards on the revamped codebase:
        ◦ Add type hints to all functions and method signatures, and ensure each public class/function has a clear docstring explaining usage .
        ◦ Configure linters/formatters: e.g. add a pyproject.toml or setup.cfg with flake8 settings, run black on the code to standardize formatting.
        ◦ Set up a basic pre-commit hook configuration for these tools so that future commits are checked (this can be included in the repository for developer convenience).
        ◦ Address any lint or type-check warnings that arise from the refactor (this improves reliability and maintainability).
        ◦ Outcome: The codebase meets PEP8 standards and passes static analysis. The use of type hints will reduce bugs and is in line with best practices .
    • Preliminary Unit Tests: Write and run initial unit tests for the refactored components:
        ◦ Focus on testing the core logic of each module: e.g. test that the prepare functionality correctly converts a sample KML to GeoJSON (using a small test file), or that the analyze computations on a small fake dataset produce expected results.
        ◦ Use pytest to structure tests. Aim to cover the critical paths of the code that might break due to refactoring. For example, if verdesat download calls Earth Engine, simulate or mock a smaller data retrieval to test the flow without hitting external API.
        ◦ If possible, test the CLI commands using a CLI runner (to ensure the argument parsing and command dispatch works after reorganization).
        ◦ These tests not only prevent regressions now but also lay the groundwork for a growing test suite.
        ◦ Outcome: A test suite (perhaps 8-10 tests initially) covering key functionalities of the refactored code. The team should achieve maybe ~50% coverage by end of Sprint 1 and have confidence that core features (prepare, download, analyze, forecast) still work as expected.
    • Documentation Updates: In parallel with development, update documentation:
        ◦ Revise the README to reflect changes: new CLI usage, new module descriptions (if any changed), and any updated installation or dependency notes.
        ◦ Add inline documentation (docstrings were added earlier) and possibly generate an API reference if relevant (could be deferred).
        ◦ Ensure the example in the Quickstart is up-to-date with the new CLI commands and has consistent output.
        ◦ Outcome: Users reading the README or using --help will get accurate guidance aligned with the refactored code. Internal code docs are improved, which will help future contributors.

By the end of Sprint 1, the VerdeSat project should have a solid, well-structured foundation. The code will be cleaner and easier to extend, the CLI more user-centric, and the groundwork laid for performance and feature enhancements in Sprint 2. Importantly, we’ll incorporate insights from external tools in our design (for example, a plugin-like architecture for data sources inspired by tools like EODAG ), ensuring future work can integrate with the broader ecosystem.

Sprint 2: Feature Expansion & Optimization (Week 2)

Sprint Focus: Building on the refactored base, this sprint adds the new data and modeling features from the roadmap while improving performance. It also increases test coverage and preps the project for future CI/CD. With another ~40 hours of team effort, we target delivering the main Phase 2 features by the end of the week.
    • Advanced Data Ingestion & Caching: Implement the next-generation data handling capabilities:
        ◦ Daily/Custom Composites: Extend the download module to support different time aggregations beyond monthly NDVI. For example, allow verdesat download to take a --frequency parameter (daily, weekly, etc.) or a date window to produce composites for that interval. This likely involves adding new Earth Engine queries or using alternative data APIs. Ensure the logic is efficient (e.g. batching requests for daily data over a period).
        ◦ In-situ Data Integration: Add functionality to ingest user-provided CSV files (e.g. sensor measurements or field data). Provide a utility that can align these with satellite data by time and location. For instance, if a CSV has coordinates and dates, the tool could retrieve corresponding satellite indices for those points. Design this in a modular way (possibly in ingestion/ module) so it can be reused by analysis or modeling code.
        ◦ Caching Layer: Develop a simple caching mechanism for downloads – e.g., maintain a local directory or database that records which satellite data (for which region/time) has been downloaded. On subsequent runs, check the cache first. This could be as straightforward as file-based caching (naming convention for files, or a JSON index) given the short sprint. The goal is to avoid re-downloading large datasets repeatedly. This aligns with the roadmap’s emphasis on caching for performance.
        ◦ Possibly incorporate an external library if it fits (for example, if using STAC API for downloads, leverage its caching or use requests caching library).
        ◦ Outcome: Users can specify fine-grained data queries (e.g. daily NDVI) and combine satellite data with their own datasets. The system will skip downloading data it already has cached, making iterative analysis faster. All these are accessible via CLI options and documented.
    • Analytics & Visualization Enhancements: Build out the analytics module to provide richer insights and interactive visuals:
        ◦ Trend Analysis & Decomposition: Implement advanced time-series analytics such as seasonal-trend decomposition (using libraries like statsmodels or custom) to help users understand long-term changes versus seasonal patterns in NDVI or other indices. Provide this as part of the verdesat analyze command (e.g., a flag --decompose to output trend/seasonal components or generate a plot).
        ◦ Interactive Dashboard Prototype: Using the webapp/ directory, create a simple dashboard (with Streamlit or Dash) that can load results from the analyses and display them. For example, a Streamlit app that lets a user select a region and then shows an NDVI time-series plot and a map of the region. This is primarily a proof-of-concept for now, but sets the stage for more polished dashboards later . Integrate this with the CLI (perhaps verdesat visualize dashboard launches the app).
        ◦ Visualization Utilities: Enhance the visualize plot functionality (if present) or create new plotting utilities in the analytics module. This could include mapping capabilities (using geopandas or contextily to show data on a map) or better time-series plotting (using matplotlib/plotly for interactive charts). The goal is to improve the out-of-the-box visual outputs for users.
        ◦ Outcome: A user can perform a full end-to-end analysis with richer outputs – e.g. decompose a time series and perhaps see the result in a quick dashboard. The groundwork for interactive use is laid, even if not fully production-ready. This demonstrates the value of the data beyond static CSV outputs.
    • Modeling Module Expansion (Forecasting): Deliver the machine learning components of the roadmap:
        ◦ Prophet Time-Series Forecasting: Implement a function or class that takes a time-series (e.g. NDVI over time for a region) and uses Facebook Prophet to forecast future values. Expose this via verdesat forecast (with Prophet as the default method). Allow the user to specify basic parameters (forecast horizon, etc.). Ensure this runs reasonably quick for a demo (Prophet is relatively fast on single series).
        ◦ LSTM Deep Learning Pipeline: Set up an LSTM-based forecasting as an alternative. This will involve preparing the data (likely scaling and windowing the time series), defining a simple neural network (could use PyTorch, since listed in dependencies), training it on the historical data, and producing a forecast. Given the time, this might be a minimal example rather than a state-of-the-art model, but it demonstrates how the module can be extended. It should be modular (e.g. a LSTMForecastModel class) so that improvements can be made later.
        ◦ Hyperparameter Tuning with Optuna: Integrate Optuna to tune either the Prophet model’s parameters or the LSTM’s hyperparameters as a demonstration. For instance, provide an optional CLI flag or a separate command like verdesat forecast optimize that runs an Optuna study to maximize forecasting accuracy by varying parameters . This likely produces some output about best parameters; focus on the integration rather than exhaustive tuning due to sprint time.
        ◦ Model Abstraction: Ensure both forecasting methods adhere to a common interface (perhaps an abstract ForecastModel class with a fit() and predict() method). This follows good OOP practice and dependency injection — the analysis pipeline can call the model through this interface without caring if it’s Prophet or LSTM . This design will also make it easier to add new models in the future (or even use openEO processes if offloading modeling to a cloud service).
        ◦ Outcome: The verdesat forecast command is significantly enhanced: users can choose between at least two forecasting methods. The modeling code is organized and maintainable, using object-oriented patterns. There is a basic demonstration of automated hyperparameter tuning. Although these models might be simple initial versions, the structure allows future refinement. Sample results (forecasts) are obtained and, if possible, visualized or saved for the user.
    • Performance Profiling & Optimization: Mid-sprint, as new features come together, profile the end-to-end workflow for inefficiencies:
        ◦ Use a few realistic test cases (maybe a moderately sized region and a year of daily data) to see how the refactored code performs. Identify slow spots such as redundant computations, slow data reads, or unoptimized loops.
        ◦ Optimize identified bottlenecks: for example, if reading many GeoTIFF files, consider using rasterio or xarray with lazy loading to only read when needed. If combining large DataFrames, ensure use of vectorized operations. If Earth Engine calls are slow, see if results can be retrieved in bulk rather than pixel-by-pixel.
        ◦ If not already done, introduce parallelism where safe – e.g. downloading multiple tiles in parallel using Python threading or asyncio (being mindful of API rate limits). Another angle is using multiprocessing for compute-heavy analysis (like running Prophet forecasts for multiple regions concurrently).
        ◦ Verify that caching (from earlier task) indeed speeds up reruns significantly by measuring with and without cache.
        ◦ Outcome: Quantitative improvement in performance (could document that “Operation X is ~Y% faster”). The application should feel snappier and handle larger input sizes than before. Any changes made for performance are tested to ensure no accuracy loss.
    • Testing, CI, and Final Polishing: In the latter half of Sprint 2, focus on solidifying quality and preparing for the next phases:
        ◦ Expanded Test Coverage: Write additional unit tests for all new features (data ingestion, caching, analytics, models). For example, simulate a small daily composite and ensure the output has the expected number of records, or test that the Prophet model produces a reasonable forecast on a toy dataset. Include edge cases (no data, invalid inputs) to improve robustness. By now, aim to reach the project goal of ≥80% test coverage .
        ◦ Continuous Integration Setup: Configure a GitHub Actions workflow (or similar CI) to automatically run linting and tests on each push/PR. This ensures the code quality remains high going forward. Although full deployment pipelines will come later, having CI for tests now is valuable.
        ◦ Documentation & Examples: Update the README and example notebooks in examples/ to cover new features. Provide an example of using the new daily composite option and the forecast command, so users can easily try them out. If major changes occurred, draft a CHANGELOG entry for this version (preparing for an eventual release).
        ◦ Review for Reusability: Do a final review to see if any functionality duplicates what existing libraries offer. For instance, if we wrote a custom download routine that could be replaced or augmented by a library like sentinelsat (for Sentinel data) or an API wrapper, note that for future improvements. The idea is to confirm that we avoided unnecessary “wheel reinvention” by leveraging tools noted in the awesome-earthobservation list and openEO where applicable (e.g., our caching and data abstractions align with patterns in those communities ).
        ◦ Outcome: The project is ready to be handed off to the next phases (CI/CD, packaging) with high confidence in its stability. The team can demonstrate a working end-to-end pipeline: preparing input data, downloading time-series, analyzing trends, visualizing results, and forecasting future changes – all with a polished CLI and documented code. Tests and CI ensure that future contributions won’t easily break existing features.

By the end of Sprint 2, VerdeSat will have evolved from an MVP to a more robust toolkit with a clean architecture and extended capabilities. The CLI will be more user-friendly, the code will be modular and performant, and new features (like finer-grained data downloads, interactive visuals, and forecasting models) will be in place. The focus on quality (testing, typing, linting) ensures that subsequent efforts (writing detailed docs, setting up full CI/CD, deploying on cloud) can proceed on a solid foundation .

The roadmap and sprints above prioritize foundational improvements first, followed by feature growth, which aligns with the project’s open-source, local-first philosophy and future ambitions. By tackling core refactoring early, the team enables rapid, confident expansion of functionality. Moreover, by reviewing external tools and standards throughout (e.g. openEO API for cloud interoperability , and the awesome EO code repository for existing solutions ), the plan ensures that VerdeSat builds on the collective knowledge of the community, accelerating development and avoiding duplicate work. With this two-sprint plan, the project is positioned to deliver significant improvements in a short time, setting the stage for future success in both local and cloud environments.

Sources:
    1. VerdeSat Repository README – project structure and roadmap 
    2. VerdeSat Repository Best Practices – coding standards to enforce (typing, testing, CI) 
    3. openEO Project – API for interoperable cloud-based Earth observation processing 
    4. Awesome Earth Observation Code – list of tools like EODAG for data access 
VerdeSat Refactoring and Expansion Roadmap

VerdeSat is a minimal Earth observation toolkit focused on sustainability, currently in an MVP state. The following roadmap and sprint plan will guide a two-person team through a comprehensive refactor and feature expansion. The primary goals are to improve modularity, adopt clean object-oriented design, enhance system architecture and performance, and make the CLI more user-friendly. Testing, documentation, and CI/CD will be addressed after the core refactoring. The plan also incorporates insights from the Earth observation open-source community to avoid reinventing the wheel.

Roadmap Stages and Milestones

Stage 1: 
Core Refactoring & Architecture Modernization

Refactor the codebase to enforce a modular, object-oriented architecture. Leverage the existing repository structure (with dedicated folders for core, ingestion, analytics, modeling, etc. ) to separate concerns and improve maintainability. Key improvements include abstracting I/O and computation layers, so that data sources (e.g. local files, Google Earth Engine, future openEO backends) and processing engines can be swapped easily. This stage establishes a clean foundation for all other enhancements.
    • Milestones: Completed reorganization of code into classes and modules (e.g. separate DataIngestion, Analysis, Modeling classes); abstract interfaces for data retrieval and computation backends (preparing for local vs cloud execution); core utilities (config, logging) centralized in the core/ module; all existing functionality still works after refactor, validated by basic tests.

Stage 2: 
CLI Usability Enhancement

Redesign the command-line interface for clarity and consistency. Currently, VerdeSat uses grouped subcommands (e.g. verdesat download timeseries, verdesat visualize plot), which will be reorganized for intuitive use. Possible changes include flattening trivial groupings or regrouping commands by domain (e.g. a top-level data command with subcommands like download, versus separate download and prepare commands). The CLI help and documentation will be improved for better guidance. Emphasis is on making common tasks easy to discover and run.
    • Milestones: New CLI command hierarchy implemented (with clear grouping or simplified commands as per the audit findings); updated help texts and usage examples; verification that all commands execute correctly with the refactored core logic; documentation (README/CLI help) reflects the new command structure.

Stage 3: 
Performance Optimization & Data Handling

Identify and address performance bottlenecks in data processing. This involves utilizing efficient libraries and patterns for geospatial data (e.g. using xarray/dask for large raster time series, vectorized operations in pandas/numpy). A caching layer will be introduced to avoid redundant data downloads and computations, improving repeat run times . The system architecture will be evaluated for concurrency opportunities (e.g. parallel downloads of tiles) and memory optimizations. This stage ensures the toolkit runs faster and can handle larger datasets smoothly.
    • Milestones: Caching mechanism implemented for satellite data (e.g. local cache of downloaded images or time-series results to reuse in future runs ); heavy computations (such as time series calculations or image processing) refactored to use optimized libraries (confirmed by performance tests showing speed improvements); capability to process daily or custom-timeframe composites added without performance degradation (leveraging chunking or parallelism as needed); no regression in results accuracy after optimization.

Stage 4: 
Data Ingestion & Analytics Expansion

Expand the range of data sources and analysis capabilities. On the data side, support will be added for daily or custom time-window composites, in addition to the current monthly NDVI (as outlined in the Phase 2 roadmap ). This means users can specify different temporal resolutions for satellite data compositing. In-situ data (e.g. CSV files of field measurements) will be ingestible and joinable with satellite data for analysis. We will also incorporate external tools where possible – for example, evaluating frameworks like EODAG for unified data search/download across providers – to avoid reinventing data access code. On the analytics side, new analysis features include advanced trend analysis (e.g. seasonal decomposition or change detection) and interactive visualization. A lightweight dashboard (using Dash or Streamlit) will be prototyped to display time-series and map visualizations, aligning with the roadmap goal of interactive analytics .
    • Milestones: Functionality to download and compose imagery at different time resolutions (e.g. daily composites) is implemented and exposed via CLI; support for reading in-situ observational CSV data and merging it with satellite time-series data; at least one interactive visualization demo (e.g. a Streamlit app in the webapp/ module) showing a sample analysis; new CLI commands or options for these features (e.g. verdesat data composite ..., verdesat analyze trends ..., or verdesat visualize dashboard) are in place and documented.

Stage 5: 
Modeling Enhancements

Introduce robust modeling capabilities for forecasting and classification as planned. This includes implementing time-series forecasting with both a machine learning approach (e.g. Facebook Prophet for univariate trends) and deep learning (e.g. an LSTM model for sequence data), in line with the Phase 2 plan . The modeling module will be structured with clean OOP design: e.g. a base Model interface with subclasses for each model type, enabling extension. Hyperparameter tuning will be integrated (using Optuna) to demonstrate automated model improvement . We will ensure models are decoupled from data ingestion through dependency injection (passing prepared data into models) as recommended in the best practices . This stage sets the groundwork for more complex modeling while keeping the code maintainable and testable.
    • Milestones: Two forecasting pipelines implemented (e.g. Prophet and a simple LSTM) and runnable on example data through the CLI (e.g. verdesat forecast --method prophet); hyperparameter tuning example integrated (e.g. an Optuna script or command to optimize a Prophet model’s parameters ); clear separation of model code from data access (models consume data inputs rather than calling I/O themselves, enabling future cloud-based data feeding); preliminary evaluation of model performance on sample datasets with results documented or plotted.

Stage 6: 
Code Quality, Testing & Documentation

Throughout the refactor and new feature implementation, maintain a strong focus on code quality. This involves adding type hints and docstrings to all public functions and classes (enforced via tools like mypy) and adhering to Python style guides with automated linters (black, flake8) . Comprehensive unit tests will be developed alongside features, aiming for ~80% coverage by the end of Stage 5 . By the conclusion of the refactoring sprints, the project should have a significantly improved README and usage documentation (though a full rewrite and tutorial can follow once features stabilize). Continuous integration (CI) will be set up to run tests and linters on each commit, preparing for future continuous deployment.
    • Milestones: Static typing checks pass (no mypy errors) on the codebase ; code passes linting/formatting checks (with a pre-commit hook configured for development) ; test suite covers major modules (target ≥80% coverage) and is integrated with a CI workflow (e.g. GitHub Actions) for automated testing; updated documentation (README and possibly a dedicated docs site) covering installation, new CLI usage, and examples.

Stage 7: 
Packaging & Future Deployment (Post-refactor)

(Planned after the initial two sprints) Prepare the project for broader use and future cloud deployment. As an open-source local-first tool, packaging the library for easy installation is important – this includes setting up versioning (semantic versioning with a CHANGELOG) and publishing to PyPI. CI/CD will be extended to handle package building and publishing when appropriate . In parallel, design considerations for cloud migration will be revisited: e.g. containerization with Docker/Helm and orchestrating tasks on cloud platforms . We will also explore aligning with openEO API standards to ensure interoperability – openEO provides a unified way to connect to various cloud-based Earth observation backends , and VerdeSat’s architecture (from Stage 1) will make it feasible to plug into such services. This stage is about transitioning from a refactored MVP into a production-ready, scalable service.
    • Milestones: PyPI package setup (project can be installed via pip); version bump and CHANGELOG entry for the refactored release; CI pipeline configured to run tests, then build and publish on new releases ; Dockerfile updated (if necessary) to reflect new architecture and tested for local deployment; a documented plan or prototype for running core processing in a cloud environment (could be as simple as a note on how to use openEO or STAC services in place of local data sources, demonstrating the flexibility of the new design ).

The above stages are prioritized in roughly the order given (refactor foundations first, then usability, performance, feature expansions, etc.). Testing and documentation improvements are woven throughout the process rather than left to the end. Packaging, CI/CD, and cloud considerations are acknowledged in design choices early on, but most implementation for those will occur after the core refactoring and feature addition sprints.

Sprint 1: Core Architecture & CLI (Week 1)

Sprint Focus: Laying the groundwork with a modular architecture and improved CLI, while maintaining existing functionality. We assume a one-week sprint (~40 hours of work for the 2-person team). Tasks are designed to be parallelizable between two developers where possible.
    • Audit & Design Planning (Day 1): Review the current MVP code structure and the recent audit findings. Identify areas of tight coupling or script-like structure that need refactoring into classes. Evaluate external resources for guidance – e.g. review the OpenEO API and Earth observation toolkits to inform our design. This research ensures we align with industry-best practices and reuse existing solutions where appropriate (for example, considering an API like openEO’s for future compatibility , or using an existing downloader library instead of custom code). Outcome: A brief design document or whiteboard outlining the new module/classes layout and CLI structure changes.
    • Refactor Core into Modules/Classes: Implement the planned restructuring of the codebase. Break up monolithic scripts into a clear module hierarchy as per the repo structure :
        ◦ Move generic utilities (config parsing, logging, file I/O) into verdesat/core/ (e.g. a ConfigManager class, Logger setup, utility functions).
        ◦ Refactor data-related logic into verdesat/ingestion/ (e.g. create a SatelliteDataIngestor class handling data download and preprocessing).
        ◦ Refactor analysis functions into verdesat/analytics/ (e.g. a class or set of functions for time-series analysis, currently used by analyze CLI).
        ◦ Ensure the new classes have clear interfaces (methods) and use initialization to manage state where appropriate (for example, a DataIngestor might be initialized with an API key or output directory).
        ◦ Implement interface/abstraction layers for key components: for instance, define an abstract base class or protocol for data retrieval so that one implementation can use Google Earth Engine now and another could use a different provider later (openEO, local files, etc.). This will make it easy to plug in new backends without altering higher-level code .
        ◦ Outcome: A cleaner project structure with core logic encapsulated in classes. Code is easier to navigate (each module focuses on a domain), and preparation is made for extending or swapping components.
    • CLI Command Reorganization: Redesign the CLI using a framework like Click or Typer (if not already in use). Incorporate the audit suggestions to improve command grouping and naming:
        ◦ Simplify where possible: If a command group has only one subcommand (e.g. verdesat download timeseries was the only download option), consider merging it (e.g. make verdesat download do the timeseries by default, with options for other types later) for a smoother UX.
        ◦ Introduce logical grouping where it helps clarity: for example, ensure that preparation steps (prepare) and data download functions are under a common theme (could be verdesat data ... group) or clearly distinct from analysis commands.
        ◦ Update the CLI help text and usage examples to reflect the new structure. Include descriptions for each command and subcommand so users understand their purpose.
        ◦ Verify that the example workflow (verdesat download ... -> verdesat analyze ... -> verdesat forecast) still works with the new CLI or update the workflow if commands changed.
        ◦ Outcome: A cleaner CLI interface that is user-friendly and consistent. Running verdesat --help should clearly list grouped commands (download/prepare, analyze/visualize, forecast, etc.) in an intuitive order.
    • Implement Logging & Config Improvements: As part of the refactor, ensure there is a unified approach to configuration and logging:
        ◦ Use a config file or object to manage parameters like default data directory, API credentials, etc., rather than scattering constants. Perhaps implement a verdesat.ini or use environment variables (with core module reading them).
        ◦ Integrate a logging framework (Python logging module) for debug/info messages instead of print statements. Provide a verbose mode flag in CLI to toggle debug logging.
        ◦ Make sure these utilities are initialized in a central place (for example, when the CLI starts, it loads config and sets up logging via the core module).
        ◦ Outcome: Consistent logging across the application (messages with timestamps/severity), and a single source of truth for configuration that can later be extended (e.g. if we add cloud configs).
    • Code Quality Pass (Linting & Typing): Begin enforcing code quality standards on the revamped codebase:
        ◦ Add type hints to all functions and method signatures, and ensure each public class/function has a clear docstring explaining usage .
        ◦ Configure linters/formatters: e.g. add a pyproject.toml or setup.cfg with flake8 settings, run black on the code to standardize formatting.
        ◦ Set up a basic pre-commit hook configuration for these tools so that future commits are checked (this can be included in the repository for developer convenience).
        ◦ Address any lint or type-check warnings that arise from the refactor (this improves reliability and maintainability).
        ◦ Outcome: The codebase meets PEP8 standards and passes static analysis. The use of type hints will reduce bugs and is in line with best practices .
    • Preliminary Unit Tests: Write and run initial unit tests for the refactored components:
        ◦ Focus on testing the core logic of each module: e.g. test that the prepare functionality correctly converts a sample KML to GeoJSON (using a small test file), or that the analyze computations on a small fake dataset produce expected results.
        ◦ Use pytest to structure tests. Aim to cover the critical paths of the code that might break due to refactoring. For example, if verdesat download calls Earth Engine, simulate or mock a smaller data retrieval to test the flow without hitting external API.
        ◦ If possible, test the CLI commands using a CLI runner (to ensure the argument parsing and command dispatch works after reorganization).
        ◦ These tests not only prevent regressions now but also lay the groundwork for a growing test suite.
        ◦ Outcome: A test suite (perhaps 8-10 tests initially) covering key functionalities of the refactored code. The team should achieve maybe ~50% coverage by end of Sprint 1 and have confidence that core features (prepare, download, analyze, forecast) still work as expected.
    • Documentation Updates: In parallel with development, update documentation:
        ◦ Revise the README to reflect changes: new CLI usage, new module descriptions (if any changed), and any updated installation or dependency notes.
        ◦ Add inline documentation (docstrings were added earlier) and possibly generate an API reference if relevant (could be deferred).
        ◦ Ensure the example in the Quickstart is up-to-date with the new CLI commands and has consistent output.
        ◦ Outcome: Users reading the README or using --help will get accurate guidance aligned with the refactored code. Internal code docs are improved, which will help future contributors.

By the end of Sprint 1, the VerdeSat project should have a solid, well-structured foundation. The code will be cleaner and easier to extend, the CLI more user-centric, and the groundwork laid for performance and feature enhancements in Sprint 2. Importantly, we’ll incorporate insights from external tools in our design (for example, a plugin-like architecture for data sources inspired by tools like EODAG ), ensuring future work can integrate with the broader ecosystem.

Sprint 2: Feature Expansion & Optimization (Week 2)

Sprint Focus: Building on the refactored base, this sprint adds the new data and modeling features from the roadmap while improving performance. It also increases test coverage and preps the project for future CI/CD. With another ~40 hours of team effort, we target delivering the main Phase 2 features by the end of the week.
    • Advanced Data Ingestion & Caching: Implement the next-generation data handling capabilities:
        ◦ Daily/Custom Composites: Extend the download module to support different time aggregations beyond monthly NDVI. For example, allow verdesat download to take a --frequency parameter (daily, weekly, etc.) or a date window to produce composites for that interval. This likely involves adding new Earth Engine queries or using alternative data APIs. Ensure the logic is efficient (e.g. batching requests for daily data over a period).
        ◦ In-situ Data Integration: Add functionality to ingest user-provided CSV files (e.g. sensor measurements or field data). Provide a utility that can align these with satellite data by time and location. For instance, if a CSV has coordinates and dates, the tool could retrieve corresponding satellite indices for those points. Design this in a modular way (possibly in ingestion/ module) so it can be reused by analysis or modeling code.
        ◦ Caching Layer: Develop a simple caching mechanism for downloads – e.g., maintain a local directory or database that records which satellite data (for which region/time) has been downloaded. On subsequent runs, check the cache first. This could be as straightforward as file-based caching (naming convention for files, or a JSON index) given the short sprint. The goal is to avoid re-downloading large datasets repeatedly. This aligns with the roadmap’s emphasis on caching for performance.
        ◦ Possibly incorporate an external library if it fits (for example, if using STAC API for downloads, leverage its caching or use requests caching library).
        ◦ Outcome: Users can specify fine-grained data queries (e.g. daily NDVI) and combine satellite data with their own datasets. The system will skip downloading data it already has cached, making iterative analysis faster. All these are accessible via CLI options and documented.
    • Analytics & Visualization Enhancements: Build out the analytics module to provide richer insights and interactive visuals:
        ◦ Trend Analysis & Decomposition: Implement advanced time-series analytics such as seasonal-trend decomposition (using libraries like statsmodels or custom) to help users understand long-term changes versus seasonal patterns in NDVI or other indices. Provide this as part of the verdesat analyze command (e.g., a flag --decompose to output trend/seasonal components or generate a plot).
        ◦ Interactive Dashboard Prototype: Using the webapp/ directory, create a simple dashboard (with Streamlit or Dash) that can load results from the analyses and display them. For example, a Streamlit app that lets a user select a region and then shows an NDVI time-series plot and a map of the region. This is primarily a proof-of-concept for now, but sets the stage for more polished dashboards later . Integrate this with the CLI (perhaps verdesat visualize dashboard launches the app).
        ◦ Visualization Utilities: Enhance the visualize plot functionality (if present) or create new plotting utilities in the analytics module. This could include mapping capabilities (using geopandas or contextily to show data on a map) or better time-series plotting (using matplotlib/plotly for interactive charts). The goal is to improve the out-of-the-box visual outputs for users.
        ◦ Outcome: A user can perform a full end-to-end analysis with richer outputs – e.g. decompose a time series and perhaps see the result in a quick dashboard. The groundwork for interactive use is laid, even if not fully production-ready. This demonstrates the value of the data beyond static CSV outputs.
    • Modeling Module Expansion (Forecasting): Deliver the machine learning components of the roadmap:
        ◦ Prophet Time-Series Forecasting: Implement a function or class that takes a time-series (e.g. NDVI over time for a region) and uses Facebook Prophet to forecast future values. Expose this via verdesat forecast (with Prophet as the default method). Allow the user to specify basic parameters (forecast horizon, etc.). Ensure this runs reasonably quick for a demo (Prophet is relatively fast on single series).
        ◦ LSTM Deep Learning Pipeline: Set up an LSTM-based forecasting as an alternative. This will involve preparing the data (likely scaling and windowing the time series), defining a simple neural network (could use PyTorch, since listed in dependencies), training it on the historical data, and producing a forecast. Given the time, this might be a minimal example rather than a state-of-the-art model, but it demonstrates how the module can be extended. It should be modular (e.g. a LSTMForecastModel class) so that improvements can be made later.
        ◦ Hyperparameter Tuning with Optuna: Integrate Optuna to tune either the Prophet model’s parameters or the LSTM’s hyperparameters as a demonstration. For instance, provide an optional CLI flag or a separate command like verdesat forecast optimize that runs an Optuna study to maximize forecasting accuracy by varying parameters . This likely produces some output about best parameters; focus on the integration rather than exhaustive tuning due to sprint time.
        ◦ Model Abstraction: Ensure both forecasting methods adhere to a common interface (perhaps an abstract ForecastModel class with a fit() and predict() method). This follows good OOP practice and dependency injection — the analysis pipeline can call the model through this interface without caring if it’s Prophet or LSTM . This design will also make it easier to add new models in the future (or even use openEO processes if offloading modeling to a cloud service).
        ◦ Outcome: The verdesat forecast command is significantly enhanced: users can choose between at least two forecasting methods. The modeling code is organized and maintainable, using object-oriented patterns. There is a basic demonstration of automated hyperparameter tuning. Although these models might be simple initial versions, the structure allows future refinement. Sample results (forecasts) are obtained and, if possible, visualized or saved for the user.
    • Performance Profiling & Optimization: Mid-sprint, as new features come together, profile the end-to-end workflow for inefficiencies:
        ◦ Use a few realistic test cases (maybe a moderately sized region and a year of daily data) to see how the refactored code performs. Identify slow spots such as redundant computations, slow data reads, or unoptimized loops.
        ◦ Optimize identified bottlenecks: for example, if reading many GeoTIFF files, consider using rasterio or xarray with lazy loading to only read when needed. If combining large DataFrames, ensure use of vectorized operations. If Earth Engine calls are slow, see if results can be retrieved in bulk rather than pixel-by-pixel.
        ◦ If not already done, introduce parallelism where safe – e.g. downloading multiple tiles in parallel using Python threading or asyncio (being mindful of API rate limits). Another angle is using multiprocessing for compute-heavy analysis (like running Prophet forecasts for multiple regions concurrently).
        ◦ Verify that caching (from earlier task) indeed speeds up reruns significantly by measuring with and without cache.
        ◦ Outcome: Quantitative improvement in performance (could document that “Operation X is ~Y% faster”). The application should feel snappier and handle larger input sizes than before. Any changes made for performance are tested to ensure no accuracy loss.
    • Testing, CI, and Final Polishing: In the latter half of Sprint 2, focus on solidifying quality and preparing for the next phases:
        ◦ Expanded Test Coverage: Write additional unit tests for all new features (data ingestion, caching, analytics, models). For example, simulate a small daily composite and ensure the output has the expected number of records, or test that the Prophet model produces a reasonable forecast on a toy dataset. Include edge cases (no data, invalid inputs) to improve robustness. By now, aim to reach the project goal of ≥80% test coverage .
        ◦ Continuous Integration Setup: Configure a GitHub Actions workflow (or similar CI) to automatically run linting and tests on each push/PR. This ensures the code quality remains high going forward. Although full deployment pipelines will come later, having CI for tests now is valuable.
        ◦ Documentation & Examples: Update the README and example notebooks in examples/ to cover new features. Provide an example of using the new daily composite option and the forecast command, so users can easily try them out. If major changes occurred, draft a CHANGELOG entry for this version (preparing for an eventual release).
        ◦ Review for Reusability: Do a final review to see if any functionality duplicates what existing libraries offer. For instance, if we wrote a custom download routine that could be replaced or augmented by a library like sentinelsat (for Sentinel data) or an API wrapper, note that for future improvements. The idea is to confirm that we avoided unnecessary “wheel reinvention” by leveraging tools noted in the awesome-earthobservation list and openEO where applicable (e.g., our caching and data abstractions align with patterns in those communities ).
        ◦ Outcome: The project is ready to be handed off to the next phases (CI/CD, packaging) with high confidence in its stability. The team can demonstrate a working end-to-end pipeline: preparing input data, downloading time-series, analyzing trends, visualizing results, and forecasting future changes – all with a polished CLI and documented code. Tests and CI ensure that future contributions won’t easily break existing features.

By the end of Sprint 2, VerdeSat will have evolved from an MVP to a more robust toolkit with a clean architecture and extended capabilities. The CLI will be more user-friendly, the code will be modular and performant, and new features (like finer-grained data downloads, interactive visuals, and forecasting models) will be in place. The focus on quality (testing, typing, linting) ensures that subsequent efforts (writing detailed docs, setting up full CI/CD, deploying on cloud) can proceed on a solid foundation .

The roadmap and sprints above prioritize foundational improvements first, followed by feature growth, which aligns with the project’s open-source, local-first philosophy and future ambitions. By tackling core refactoring early, the team enables rapid, confident expansion of functionality. Moreover, by reviewing external tools and standards throughout (e.g. openEO API for cloud interoperability , and the awesome EO code repository for existing solutions ), the plan ensures that VerdeSat builds on the collective knowledge of the community, accelerating development and avoiding duplicate work. With this two-sprint plan, the project is positioned to deliver significant improvements in a short time, setting the stage for future success in both local and cloud environments.

Sources:
    1. VerdeSat Repository README – project structure and roadmap 
    2. VerdeSat Repository Best Practices – coding standards to enforce (typing, testing, CI) 
    3. openEO Project – API for interoperable cloud-based Earth observation processing 
    4. Awesome Earth Observation Code – list of tools like EODAG for data access 
Below is a complete mapping of every major free‐floating function and CLI command in your repo to its new home in the class-based design. Use this as your migration checklist—once you’ve moved the body into the method shown, you can delete the old function.
Old Function / CLI
File (approx.)
New Class & Method
EE Initinitialize(cred_path, project)
ingestion/downloader.py
EarthEngineManager.initialize(cred_path: str, project: str) -> None
EE getInfosafe_get_info(obj, max_retries)
ingestion/downloader.py
EarthEngineManager.safe_get_info(obj: EEObject, retries: int) -> Any
EE Collectionget_image_collection(...)
ingestion/downloader.py
EarthEngineManager.get_image_collection(region: AOI, mask_clouds: bool) -> ee.ImageCollection
Cloud maskmask_fmask_bits(img)
ingestion/mask.py
SensorSpec.cloud_mask(img: ee.Image) -> ee.Image
Mask helpermask_clouds(img)
ingestion/mask.py
(alias) or static <SensorSpec>.cloud_mask
NDVI/EVIcompute_ndvi(img)
ingestion/indices.py
IndexRegistry.compute("ndvi", img: ee.Image) -> ee.Image  or SensorSpec.compute_index(img, "ndvi")
All indicescompute_index(img, index)
ingestion/indices.py
SensorSpec.compute_index(img: ee.Image, index: str) -> ee.Image
Get compositeget_composite(…)
ingestion/chips.py
DataIngestor.get_composite(aoi: AOI, index: str, period: str) -> ee.Image
Download chipsdownload_chips(…)
ingestion/chips.py & core/cli.py:download chips
DataIngestor.download_chips(aoi: AOI, index: str, period: str, out_dir: Path, bands: List[str]) -> List[Path]
Export GeoTIFF/PNGexport_chips(…)
ingestion/chips.py
DataIngestor.export_chips(... same args ...) -> List[Path]
Shapefile → AOIsShapeFilePreprocessor
ingestion/shapefile_preprocessor.py
AOI.from_directory(path: str) -> List[AOI]  (factory method)
Load GeoJSONload_geojson(path)
core/cli.py
AOI.from_geojson(path: str) -> AOI  (factory method)
Prepare CLIprepare(input_dir)
core/cli.py:prepare
VectorPreprocessor.process(input_dir: str) -> List[AOI]  (or reuse AOI.from_directory)
TS download & aggrdownload_ndvi_timeseries
analytics/timeseries.py & core/cli.py:timeseries
DataIngestor.download_timeseries(aoi, start, end, scale, index, freq) -> pd.DataFrame
Aggregateaggregate_timeseries(df,freq,index)
analytics/timeseries.py
AnalyticsEngine.aggregate(df: pd.DataFrame, freq: str, index: str) -> pd.DataFrame  orTimeSeries.aggregate(freq)
Fill gapsyour_fill_function(df, method)
analytics/preprocessing.py & core/cli.py:preprocess
TimeSeries.fill_gaps(method: str) -> TimeSeries
Decomposedecompose(df, period)
analytics/decomposition.py & core/cli.py:decompose
TimeSeries.seasonal_decompose(period: Optional[int]) -> DecomposeResult
Trendcompute_trend(df, col, id_col)
analytics/trend.py & core/cli.py:trend
AnalyticsEngine.compute_trend(ts: TimeSeries) -> TrendResult
Statscompute_stats(df)
analytics/stats.py & core/cli.py:stats
AnalyticsEngine.compute_stats(ts: TimeSeries) -> StatsResult
Daily TSdaily_timeseries(...)
analytics/timeseries.py
Merged into DataIngestor.download_timeseries(...)
Monthly TSmonthly_timeseries(...)
analytics/timeseries.py
Merged into DataIngestor.download_timeseries(...)
Forecast CLIforecast(...)
core/cli.py:forecast
ProphetModel.fit(ts); ProphetModel.predict(horizon)LSTMModel.fit(ts); LSTMModel.predict(horizon)
Animateanimate(...)
visualization/animate.py & core/cli.py:visualize animate
Visualizer.save_gif(image_paths: List[Path], out_path: Path, **opts)
Plot (static)plot(...)
visualization/static_viz.py & core/cli.py:visualize plot
Visualizer.plot_static(ts: TimeSeries, out_path: Path, **opts)
Plot (interactive)plotly_viz.plot(...)
visualization/plotly_viz.py
Visualizer.plot_interactive(ts: TimeSeries, out_path: Path, **opts)
Reportgenerate_report(...)
visualization/report.py & core/cli.py:report
Visualizer.generate_report(project: VerdeSatProject, out_path: Path, **opts)
Pipelinepipeline_report(...)
core/cli.py:pipeline
VerdeSatProject.run_pipeline(**params) -> None  (orchestrates DataIngestor → AnalyticsEngine → Visualizer)
Any misc helper
scattered in core/utils.py or inline
move into core/utils.py or appropriate class as @staticmethod or free func in utils.py

How to use this mapping
    1. Create the stub in the new class/method signature.
    2. Copy the function body into that method, replacing global parameters with self. or explicit arguments.
    3. Update imports in core/cli.py to call Class.method(...) instead of the old function.
    4. Write a quick unit test for the new method to ensure parity.
    5. Remove the old standalone function.

With this checklist you’ll cover every existing endpoint in your MVP. Let me know if you spot any function or helper I missed, and I’ll slot it into its class right away!
